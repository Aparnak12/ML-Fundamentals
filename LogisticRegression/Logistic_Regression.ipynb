{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPBH4r4uDjKQIgiGSCy2ZuN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "OjXw_U5my_9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickleFile = open(\"hw2_p3.pkl\",\"rb\")\n",
        "data = pd.read_pickle(pickleFile)\n",
        "#data"
      ],
      "metadata": {
        "id": "sRWBSCvjzFFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = data['x_train']\n",
        "x_test = data['x_test']\n",
        "\n",
        "y_train = data['y_train']\n",
        "y_test = data['y_test']"
      ],
      "metadata": {
        "id": "PdnEP8ZozSk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_scaled = StandardScaler().fit_transform(x_train)\n",
        "X_test_scaled = StandardScaler().fit_transform(x_test)"
      ],
      "metadata": {
        "id": "D607JWM21rXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Choice of Basis (Part-1)\n",
        "\n",
        "For the given data, we can choose a polynomial basis function of degree $2$, i.e if $x$ = $[$$x_{1}$, $x_{2}$$]$, then the basis function is: Ï• = $[$$1$, $x_{1}$, $x_{2}$, $x_{1}^2$, $x_{2}^2$, $x_{1} x_{2}$$]$.\n",
        "\n",
        "Therefore, the equation to fit is:\n",
        "$y = c_0 + c_1 x_{1} + c_2 x_{2} + c_3 x_{1}^2 + c_4 x_{2}^2 + c_5 x_{1} x_{2}$\n",
        "\n",
        "The reasons for choosing polynomial basis function for nonlinear logistic regression are as follows:\n",
        "1. Polynomial basis functions allow logistic regression to establish non-linear decision boundaries, making it possible to deal with datasets that have a non-linear structure and where classes are not linearly separable.\n",
        "\n",
        "2. Polynomial transformations generate interaction terms and higher-order terms of the original features, enabling the model to capture more complex relationships and interactions between different features, which can be critical in understanding the underlying data patterns.\n",
        "\n",
        "3. Introducing polynomial features increases the flexibility of the logistic regression model, enabling it to adapt better to the underlying patterns in the data, potentially leading to better classification accuracy.\n"
      ],
      "metadata": {
        "id": "VH2BL3ACoqz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "poly = PolynomialFeatures(degree=2)\n",
        "X_train_poly = poly.fit_transform(X_train_scaled)\n",
        "X_test_poly = poly.transform(X_test_scaled)"
      ],
      "metadata": {
        "id": "Ygxda0MVztaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Choosing Epochs and Learning Rate; Regression helper functions"
      ],
      "metadata": {
        "id": "yHMRKCbEw2pS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 150\n",
        "lr = 0.1"
      ],
      "metadata": {
        "id": "IXt7q-mWloMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "  return np.array([sigmoid_function(value) for value in x])\n",
        "\n",
        "def sigmoid_function(x):\n",
        "  if x >= 0:\n",
        "      z = np.exp(-x)\n",
        "      return 1 / (1 + z)\n",
        "  else:\n",
        "      z = np.exp(x)\n",
        "      return z / (1 + z)"
      ],
      "metadata": {
        "id": "vfL3EYSaxthP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradients(x, y_true, y_pred):\n",
        "  diff =  y_pred - y_true\n",
        "  gradients_w = np.matmul(x.transpose(), diff)\n",
        "  gradients_w = np.array([np.mean(grad) for grad in gradients_w])\n",
        "  return gradients_w"
      ],
      "metadata": {
        "id": "bQw6SG2xyvxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Binary Classification (Part-2)"
      ],
      "metadata": {
        "id": "PoGz4ImPuT9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def binaryFit(x, y, x_test):\n",
        "  weights = np.zeros(x.shape[1])\n",
        "  for i in range(epochs):\n",
        "    pred = sigmoid(np.matmul(weights, x.transpose()))\n",
        "    error_w = compute_gradients(x, y, pred)\n",
        "    weights -= lr * error_w\n",
        "  probabilities = sigmoid(np.matmul(x_test, weights.transpose()))\n",
        "  return [1 if p >= 0.5 else 0 for p in probabilities]"
      ],
      "metadata": {
        "id": "u-Q_Kv0t37Ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = binaryFit(X_train_poly, y_train, X_test_poly)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test accuracy for binary classification is\",accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAMIKnKRzsZh",
        "outputId": "9e2416c7-c4a2-4884-deb7-c08e61561c9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy for binary classification is 0.3333333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Multi-Class Classification (Part-3)"
      ],
      "metadata": {
        "id": "IVCZmRCpuZWL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To adapt the binary logistic regression to a multi-class classification problem, we can use the One-vs-Rest (OvR) strategy. In this approach, for $k$ classes, we train $k$ binary classifiers. For each classifier, one class is treated as the positive class $($$1$$)$, and all other classes are treated as the negative class $($$0$$)$. When making predictions, we choose the class that corresponds to the classifier with the highest output probability."
      ],
      "metadata": {
        "id": "f5HBFXFXu1Pb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def multiFit(x, y, x_test):\n",
        "  classes = np.unique(y)\n",
        "  weights_list = []\n",
        "  for cls in classes:\n",
        "    binary_y = np.where(y == cls, 1, 0)\n",
        "    weights = np.zeros(x.shape[1])\n",
        "    for i in range(epochs):\n",
        "      pred = sigmoid(np.matmul(weights, x.transpose()))\n",
        "      error_w = compute_gradients(x, binary_y, pred)\n",
        "      weights -= lr * error_w\n",
        "    weights_list.append(weights)\n",
        "  probabilities_list = [sigmoid(np.matmul(x_test, weights.transpose())) for weights in weights_list]\n",
        "  probabilities = np.vstack(probabilities_list).T\n",
        "  predictions = np.argmax(probabilities, axis=1)\n",
        "  return predictions"
      ],
      "metadata": {
        "id": "qmn1Ma0Jaepy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = multiFit(X_train_poly, y_train, X_test_poly)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test accuracy for multi-class classification is\",accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_boOe7GabA62",
        "outputId": "7b39848b-8fc3-4119-cdd0-32ca54007454"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy for multi-class classification is 0.9933333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Final Test Accuracy (Part-4)\n",
        "\n",
        "As we can see, the final test accuracy for multi-class classification is $99$%. The accuracy was much lower at $33$% for binary classification as we attempted to fit data with more than two classes into a binary classification model.\n"
      ],
      "metadata": {
        "id": "QeOxTK5AvZPq"
      }
    }
  ]
}
