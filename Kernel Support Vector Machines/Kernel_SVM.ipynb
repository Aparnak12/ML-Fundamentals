{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from cvxopt import matrix, solvers\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "KvPuhFL-883S"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickleFile = open(\"hw2_p3.pkl\",\"rb\")\n",
        "data = pd.read_pickle(pickleFile)\n",
        "#data"
      ],
      "metadata": {
        "id": "IY3rmXr5aAbi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = data['x_train']\n",
        "x_test = data['x_test']\n",
        "y_train = data['y_train']\n",
        "y_test = data['y_test']"
      ],
      "metadata": {
        "id": "ZGhKe7JfZ4c1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.2\n",
        "\n",
        "Part 1:\n",
        "\n",
        "Radial basis function (RBF) kernels are a popular choice for kernel support vector machines (SVMs) for classification problems because they offer a number of advantages: Allows non-linear relationship, is more computationally efficient, has good robustness to noise, universal approximation, offers better performance, flexible in tuning, easy to interpret and understand. RBF kernels are a powerful and versatile tool for solving classification problems, especially those involving complex, non-linear data."
      ],
      "metadata": {
        "id": "SRFq4WvG9oTX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.2\n",
        "\n",
        "Part 2"
      ],
      "metadata": {
        "id": "x_CN9O5r_tpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomSVM:\n",
        "    def __init__(self, C_param=1.0, kernel_type='rbf', gamma_param=0.1):\n",
        "        self.C_param = C_param\n",
        "        self.kernel_type = kernel_type\n",
        "        self.gamma_param = gamma_param\n",
        "        self.alpha_values = None\n",
        "        self.bias_term = None\n",
        "        self.support_vectors = None\n",
        "        self.support_vector_labels = None\n",
        "\n",
        "    def _custom_kernel(self, data1, data2):\n",
        "        if self.kernel_type == 'rbf':\n",
        "            return np.exp(-self.gamma_param * np.linalg.norm(data1 - data2)**2)\n",
        "\n",
        "    def _construct_p_matrix(self, data, labels):\n",
        "        n_samples, n_features = data.shape\n",
        "        P_matrix = np.zeros((n_samples, n_samples))\n",
        "        for i in range(n_samples):\n",
        "            for j in range(n_samples):\n",
        "                P_matrix[i, j] = labels[i] * labels[j] * self._custom_kernel(data[i], data[j])\n",
        "        return P_matrix\n",
        "\n",
        "    def fit(self, data, labels):\n",
        "        n_samples, n_features = data.shape\n",
        "\n",
        "        P_matrix = matrix(self._construct_p_matrix(data, labels))\n",
        "        q_matrix = matrix(-np.ones(n_samples))\n",
        "        G_matrix = matrix(np.vstack((-np.eye(n_samples), np.eye(n_samples))))\n",
        "        h_vector = matrix(np.hstack((np.zeros(n_samples), self.C_param * np.ones(n_samples))))\n",
        "        A_matrix = matrix(labels.reshape(1, -1), (1, n_samples), 'd')\n",
        "        b_vector = matrix(0.0)\n",
        "        solution = solvers.qp(P_matrix, q_matrix, G_matrix, h_vector, A_matrix, b_vector)\n",
        "        self.alpha_values = np.array(solution['x']).flatten()\n",
        "        sv_indices = np.where(self.alpha_values > 1e-5)[0]\n",
        "        self.support_vectors = data[sv_indices]\n",
        "        self.support_vector_labels = labels[sv_indices]\n",
        "        self.bias_term = np.mean(\n",
        "            [labels[i] - np.sum(self.alpha_values * labels * self._custom_kernel(data, data[i])) for i in sv_indices]\n",
        "        )\n",
        "\n",
        "    def predict(self, data_input):\n",
        "        if self.alpha_values is None or self.bias_term is None:\n",
        "            raise ValueError(\"Train model before making predictions.\")\n",
        "\n",
        "        n_samples_input = data_input.shape[0]\n",
        "        predictions_output = np.zeros(n_samples_input)\n",
        "\n",
        "        for i in range(n_samples_input):\n",
        "            prediction = 0\n",
        "            for j in range(len(self.support_vectors)):\n",
        "                prediction += self.alpha_values[j] * self.support_vector_labels[j] * self._custom_kernel(self.support_vectors[j], data_input[i])\n",
        "            predictions_output[i] = prediction + self.bias_term\n",
        "\n",
        "        return np.sign(predictions_output)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fqbSMCmGCWu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.2\n",
        "\n",
        "Part 3"
      ],
      "metadata": {
        "id": "mGdt1FtjADTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomMultiClassSVM:\n",
        "    def __init__(self, C_param=1.0, kernel_type='rbf', gamma_param=0.1):\n",
        "        self.C_param = C_param\n",
        "        self.kernel_type = kernel_type\n",
        "        self.gamma_param = gamma_param\n",
        "        self.binary_classifiers = {}\n",
        "\n",
        "    def fit(self, data_input, labels_input):\n",
        "        unique_classes = np.unique(labels_input)\n",
        "\n",
        "        for i, class_label in enumerate(unique_classes):\n",
        "            binary_labels = np.where(labels_input == class_label, 1, -1)\n",
        "            binary_svm = CustomSVM(C_param=self.C_param, kernel_type=self.kernel_type, gamma_param=self.gamma_param)\n",
        "            binary_svm.fit(data_input, binary_labels)\n",
        "            self.binary_classifiers[class_label] = binary_svm\n",
        "\n",
        "    def predict(self, data_input):\n",
        "        class_scores = np.zeros((data_input.shape[0], len(self.binary_classifiers)))\n",
        "\n",
        "        for i, class_label in enumerate(self.binary_classifiers):\n",
        "            binary_svm = self.binary_classifiers[class_label]\n",
        "            class_scores[:, i] = binary_svm.predict_raw(data_input)\n",
        "\n",
        "        # Assign the class with the highest score as the predicted class\n",
        "        predictions_output = np.argmax(class_scores, axis=1)\n",
        "\n",
        "        return predictions_output\n",
        "\n",
        "class CustomSVM:\n",
        "    def __init__(self, C_param=1.0, kernel_type='rbf', gamma_param=0.1):\n",
        "        self.C_param = C_param\n",
        "        self.kernel_type = kernel_type\n",
        "        self.gamma_param = gamma_param\n",
        "        self.alpha_values = None\n",
        "        self.bias_term = None\n",
        "        self.support_vectors = None\n",
        "        self.support_vector_labels = None\n",
        "\n",
        "    def _custom_kernel(self, data1, data2):\n",
        "        if self.kernel_type == 'rbf':\n",
        "            return np.exp(-self.gamma_param * np.linalg.norm(data1 - data2)**2)\n",
        "\n",
        "    def fit(self, data_input, labels_input):\n",
        "        n_samples_input, n_features_input = data_input.shape\n",
        "        P_matrix = matrix(np.outer(labels_input, labels_input) *\n",
        "                          np.array([[self._custom_kernel(data_input[i], data_input[j]) for j in range(n_samples_input)] for i in range(n_samples_input)]))\n",
        "        q_matrix = matrix(-np.ones(n_samples_input))\n",
        "        G_matrix = matrix(np.vstack((-np.eye(n_samples_input), np.eye(n_samples_input))))\n",
        "        h_vector = matrix(np.hstack((np.zeros(n_samples_input), self.C_param * np.ones(n_samples_input))))\n",
        "        A_matrix = matrix(labels_input.reshape(1, -1), (1, n_samples_input), 'd')\n",
        "        b_vector = matrix(0.0)\n",
        "        solution = solvers.qp(P_matrix, q_matrix, G_matrix, h_vector, A_matrix, b_vector)\n",
        "\n",
        "        self.alpha_values = np.array(solution['x']).flatten()\n",
        "\n",
        "        sv_indices = np.where(self.alpha_values > 1e-5)[0]\n",
        "        self.support_vectors = data_input[sv_indices]\n",
        "        self.support_vector_labels = labels_input[sv_indices]\n",
        "\n",
        "        self.bias_term = np.mean(\n",
        "            [labels_input[i] - np.sum(self.alpha_values * labels_input * self._custom_kernel(data_input, data_input[i])) for i in sv_indices]\n",
        "        )\n",
        "\n",
        "    def predict_raw(self, data_input):\n",
        "        if self.alpha_values is None or self.bias_term is None:\n",
        "            raise ValueError(\"Train model before making predictions.\")\n",
        "\n",
        "        n_samples_input = data_input.shape[0]\n",
        "        predictions_output = np.zeros(n_samples_input)\n",
        "\n",
        "        for i in range(n_samples_input):\n",
        "            prediction = 0\n",
        "            for j in range(len(self.support_vectors)):\n",
        "                prediction += self.alpha_values[j] * self.support_vector_labels[j] * self._custom_kernel(self.support_vectors[j], data_input[i])\n",
        "            predictions_output[i] = prediction + self.bias_term\n",
        "\n",
        "        return predictions_output"
      ],
      "metadata": {
        "id": "NFiAQun3D3aQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.2\n",
        "\n",
        "Part 4\n"
      ],
      "metadata": {
        "id": "U7Q7tyVnAOda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomSVM:\n",
        "    def __init__(self, C_param=1.0, kernel_type='rbf', gamma_param=0.1, degree_param=3, coef0_param=0.0):\n",
        "        self.C_param = C_param\n",
        "        self.kernel_type = kernel_type\n",
        "        self.gamma_param = gamma_param\n",
        "        self.degree_param = degree_param\n",
        "        self.coef0_param = coef0_param\n",
        "        self.alpha_values = None\n",
        "        self.bias_term = None\n",
        "        self.support_vectors = None\n",
        "        self.support_vector_labels = None\n",
        "\n",
        "    def _custom_kernel(self, data1, data2):\n",
        "        if self.kernel_type == 'linear':\n",
        "            return np.dot(data1, data2)\n",
        "        elif self.kernel_type == 'poly':\n",
        "            return (np.dot(data1, data2) + self.coef0_param) ** self.degree_param\n",
        "        elif self.kernel_type == 'rbf':\n",
        "            return np.exp(-self.gamma_param * np.linalg.norm(data1 - data2)**2)\n",
        "\n",
        "    def fit(self, data_input, labels_input):\n",
        "        n_samples_input, n_features_input = data_input.shape\n",
        "\n",
        "        P_matrix = matrix(np.outer(labels_input, labels_input) *\n",
        "                          np.array([[self._custom_kernel(data_input[i], data_input[j]) for j in range(n_samples_input)] for i in range(n_samples_input)]))\n",
        "        q_matrix = matrix(-np.ones(n_samples_input))\n",
        "        G_matrix = matrix(np.vstack((-np.eye(n_samples_input), np.eye(n_samples_input))))\n",
        "        h_vector = matrix(np.hstack((np.zeros(n_samples_input), self.C_param * np.ones(n_samples_input))))\n",
        "        A_matrix = matrix(labels_input.reshape(1, -1), (1, n_samples_input), 'd')\n",
        "        b_vector = matrix(0.0)\n",
        "        solution = solvers.qp(P_matrix, q_matrix, G_matrix, h_vector, A_matrix, b_vector)\n",
        "\n",
        "        self.alpha_values = np.array(solution['x']).flatten()\n",
        "\n",
        "        sv_indices = np.where(self.alpha_values > 1e-5)[0]\n",
        "        self.support_vectors = data_input[sv_indices]\n",
        "        self.support_vector_labels = labels_input[sv_indices]\n",
        "\n",
        "        self.bias_term = np.mean(\n",
        "            [labels_input[i] - np.sum(self.alpha_values * labels_input * self._custom_kernel(data_input, data_input[i])) for i in sv_indices]\n",
        "        )\n",
        "\n",
        "    def predict_raw(self, data_input):\n",
        "        if self.alpha_values is None or self.bias_term is None:\n",
        "            raise ValueError(\"Train model before making predictions.\")\n",
        "\n",
        "        n_samples_input = data_input.shape[0]\n",
        "        predictions_output = np.zeros(n_samples_input)\n",
        "\n",
        "        for i in range(n_samples_input):\n",
        "            prediction = 0\n",
        "            for j in range(len(self.support_vectors)):\n",
        "                prediction += self.alpha_values[j] * self.support_vector_labels[j] * self._custom_kernel(self.support_vectors[j], data_input[i])\n",
        "            predictions_output[i] = prediction + self.bias_term\n",
        "\n",
        "        return predictions_output\n"
      ],
      "metadata": {
        "id": "P-I7HoGR_heV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomMultiClassSVM:\n",
        "    def __init__(self, C_param=1.0, kernel_type='rbf', gamma_param=0.1, degree_param=3, coef0_param=0.0):\n",
        "        self.C_param = C_param\n",
        "        self.kernel_type = kernel_type\n",
        "        self.gamma_param = gamma_param\n",
        "        self.degree_param = degree_param\n",
        "        self.coef0_param = coef0_param\n",
        "        self.classifiers = {}\n",
        "\n",
        "    def fit(self, data_input, labels_input):\n",
        "        unique_classes = np.unique(labels_input)\n",
        "\n",
        "        for i, class_label in enumerate(unique_classes):\n",
        "            binary_labels = np.where(labels_input == class_label, 1, -1)\n",
        "            binary_svm = CustomSVM(C_param=self.C_param, kernel_type=self.kernel_type, gamma_param=self.gamma_param, degree_param=self.degree_param, coef0_param=self.coef0_param)\n",
        "            binary_svm.fit(data_input, binary_labels)\n",
        "            self.classifiers[class_label] = binary_svm\n",
        "\n",
        "    def predict(self, data_input):\n",
        "        class_scores = np.zeros((data_input.shape[0], len(self.classifiers)))\n",
        "\n",
        "        for i, class_label in enumerate(self.classifiers):\n",
        "            classifier = self.classifiers[class_label]\n",
        "            class_scores[:, i] = classifier.predict_raw(data_input)\n",
        "\n",
        "        predictions_output = np.argmax(class_scores, axis=1)\n",
        "\n",
        "        return predictions_output\n"
      ],
      "metadata": {
        "id": "aw05LBd-IJgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# When kernel is RBF\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "multiclass_svm_custom = CustomMultiClassSVM(C_param=1.0, kernel_type='rbf')\n",
        "multiclass_svm_custom.fit(X_train, y_train)\n",
        "predictions_custom = multiclass_svm_custom.predict(x_test)\n",
        "\n",
        "# Calculate the test accuracy\n",
        "test_accuracy = accuracy_score(y_test, predictions_custom)\n",
        "\n",
        "print(f\"Test Accuracy Obatined for radial basis function: {test_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMhmfKggILbs",
        "outputId": "1476e327-1fb9-4e50-e14f-079ff745c265"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     pcost       dcost       gap    pres   dres\n",
            " 0: -4.1678e+01 -8.5546e+02  4e+03  2e+00  2e-15\n",
            " 1: -2.1866e+01 -3.7379e+02  5e+02  1e-01  2e-15\n",
            " 2: -2.6698e+01 -8.3107e+01  6e+01  1e-02  2e-15\n",
            " 3: -3.3279e+01 -6.2754e+01  3e+01  5e-03  1e-15\n",
            " 4: -3.7476e+01 -4.9921e+01  1e+01  2e-03  1e-15\n",
            " 5: -3.9313e+01 -4.5343e+01  6e+00  7e-04  1e-15\n",
            " 6: -4.0606e+01 -4.2492e+01  2e+00  1e-04  2e-15\n",
            " 7: -4.1072e+01 -4.1613e+01  5e-01  2e-05  2e-15\n",
            " 8: -4.1240e+01 -4.1349e+01  1e-01  1e-06  2e-15\n",
            " 9: -4.1265e+01 -4.1316e+01  5e-02  9e-08  2e-15\n",
            "10: -4.1285e+01 -4.1292e+01  6e-03  9e-09  2e-15\n",
            "11: -4.1288e+01 -4.1288e+01  1e-04  1e-10  2e-15\n",
            "12: -4.1288e+01 -4.1288e+01  2e-06  2e-12  2e-15\n",
            "Optimal solution found.\n",
            "     pcost       dcost       gap    pres   dres\n",
            " 0: -3.8454e+01 -8.1647e+02  3e+03  2e+00  2e-15\n",
            " 1: -1.9536e+01 -3.4221e+02  4e+02  8e-02  2e-15\n",
            " 2: -2.5789e+01 -8.4795e+01  7e+01  1e-02  2e-15\n",
            " 3: -3.4007e+01 -5.4600e+01  2e+01  3e-03  2e-15\n",
            " 4: -3.6769e+01 -4.6965e+01  1e+01  1e-03  2e-15\n",
            " 5: -3.8171e+01 -4.2964e+01  5e+00  3e-04  2e-15\n",
            " 6: -3.9117e+01 -4.1086e+01  2e+00  5e-05  2e-15\n",
            " 7: -3.9575e+01 -4.0309e+01  7e-01  1e-05  2e-15\n",
            " 8: -3.9793e+01 -3.9964e+01  2e-01  1e-06  2e-15\n",
            " 9: -3.9855e+01 -3.9882e+01  3e-02  1e-15  2e-15\n",
            "10: -3.9866e+01 -3.9870e+01  4e-03  2e-16  2e-15\n",
            "11: -3.9868e+01 -3.9868e+01  6e-04  5e-15  2e-15\n",
            "12: -3.9868e+01 -3.9868e+01  3e-05  5e-15  2e-15\n",
            "Optimal solution found.\n",
            "     pcost       dcost       gap    pres   dres\n",
            " 0: -3.0718e+01 -8.3759e+02  3e+03  2e+00  1e-15\n",
            " 1: -1.2555e+01 -3.5059e+02  4e+02  4e-02  2e-15\n",
            " 2: -2.4429e+01 -8.6022e+01  6e+01  7e-03  1e-15\n",
            " 3: -3.1314e+01 -5.5079e+01  2e+01  2e-03  1e-15\n",
            " 4: -3.4506e+01 -4.4806e+01  1e+01  5e-04  1e-15\n",
            " 5: -3.6165e+01 -4.0896e+01  5e+00  1e-04  1e-15\n",
            " 6: -3.7092e+01 -3.8716e+01  2e+00  2e-05  1e-15\n",
            " 7: -3.7468e+01 -3.8051e+01  6e-01  5e-06  1e-15\n",
            " 8: -3.7620e+01 -3.7812e+01  2e-01  1e-14  2e-15\n",
            " 9: -3.7705e+01 -3.7714e+01  9e-03  8e-15  1e-15\n",
            "10: -3.7709e+01 -3.7709e+01  1e-04  1e-14  2e-15\n",
            "11: -3.7709e+01 -3.7709e+01  2e-06  2e-14  1e-15\n",
            "Optimal solution found.\n",
            "Test Accuracy Obatined for radial basis function: 59.33%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# When Kernel is a linear function\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "multiclass_svm_custom = CustomMultiClassSVM(C_param=1.0, kernel_type='linear')\n",
        "multiclass_svm_custom.fit(X_train, y_train)\n",
        "predictions_custom = multiclass_svm_custom.predict(x_test)\n",
        "\n",
        "# Calculate the test accuracy\n",
        "test_accuracy = accuracy_score(y_test, predictions_custom)\n",
        "\n",
        "print(f\"Test Accuracy Obtained for linear function: {test_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmAsG_HfJ7xY",
        "outputId": "8a059d21-d65c-4106-e6c7-be1115c31bba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     pcost       dcost       gap    pres   dres\n",
            " 0: -5.3311e+02 -1.3459e+03  4e+03  2e+00  1e-12\n",
            " 1: -3.8082e+02 -8.7998e+02  5e+02  2e-14  2e-12\n",
            " 2: -3.9923e+02 -4.0876e+02  1e+01  1e-14  1e-12\n",
            " 3: -3.9999e+02 -4.0009e+02  1e-01  9e-14  1e-12\n",
            " 4: -4.0000e+02 -4.0000e+02  1e-03  4e-14  1e-12\n",
            " 5: -4.0000e+02 -4.0000e+02  1e-05  2e-14  1e-12\n",
            "Optimal solution found.\n",
            "     pcost       dcost       gap    pres   dres\n",
            " 0: -5.3333e+02 -1.3365e+03  4e+03  2e+00  1e-12\n",
            " 1: -3.8146e+02 -8.7224e+02  5e+02  3e-14  2e-12\n",
            " 2: -3.9928e+02 -4.0824e+02  9e+00  3e-14  2e-12\n",
            " 3: -3.9999e+02 -4.0008e+02  9e-02  1e-13  2e-12\n",
            " 4: -4.0000e+02 -4.0000e+02  9e-04  1e-14  1e-12\n",
            " 5: -4.0000e+02 -4.0000e+02  9e-06  4e-14  1e-12\n",
            "Optimal solution found.\n",
            "     pcost       dcost       gap    pres   dres\n",
            " 0: -5.3316e+02 -1.3570e+03  4e+03  2e+00  1e-12\n",
            " 1: -3.7999e+02 -8.8908e+02  5e+02  4e-14  1e-12\n",
            " 2: -3.9898e+02 -4.1483e+02  2e+01  7e-15  1e-12\n",
            " 3: -3.9999e+02 -4.0015e+02  2e-01  1e-13  1e-12\n",
            " 4: -4.0000e+02 -4.0000e+02  2e-03  1e-13  1e-12\n",
            " 5: -4.0000e+02 -4.0000e+02  2e-05  2e-14  1e-12\n",
            "Optimal solution found.\n",
            "Test Accuracy Obtained for linear function: 33.33%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# When kernel is a poly function\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "multiclass_svm_custom = CustomMultiClassSVM(C_param=1.0, kernel_type='poly')\n",
        "multiclass_svm_custom.fit(X_train, y_train)\n",
        "predictions_custom = multiclass_svm_custom.predict(x_test)\n",
        "\n",
        "# Calculate the test accuracy\n",
        "test_accuracy = accuracy_score(y_test, predictions_custom)\n",
        "\n",
        "print(f\"Test Accuracy Obatined for poly function: {test_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_geBajWYWJ4v",
        "outputId": "3c583131-309a-442d-fd0c-2439d2c2f648"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     pcost       dcost       gap    pres   dres\n",
            " 0: -5.3288e+02 -1.3407e+03  4e+03  2e+00  1e-09\n",
            " 1: -3.8106e+02 -8.7574e+02  5e+02  2e-13  1e-09\n",
            " 2: -3.9926e+02 -4.0841e+02  9e+00  1e-14  1e-09\n",
            " 3: -3.9999e+02 -4.0008e+02  9e-02  8e-14  2e-09\n",
            " 4: -4.0000e+02 -4.0000e+02  9e-04  9e-14  1e-09\n",
            " 5: -4.0000e+02 -4.0000e+02  1e-05  8e-15  1e-09\n",
            "Optimal solution found.\n",
            "     pcost       dcost       gap    pres   dres\n",
            " 0: -5.3317e+02 -1.3399e+03  4e+03  2e+00  2e-09\n",
            " 1: -3.8121e+02 -8.7504e+02  5e+02  7e-14  2e-09\n",
            " 2: -3.9927e+02 -4.0830e+02  9e+00  7e-14  2e-09\n",
            " 3: -3.9999e+02 -4.0008e+02  9e-02  6e-15  2e-09\n",
            " 4: -4.0000e+02 -4.0000e+02  9e-04  3e-14  2e-09\n",
            " 5: -4.0000e+02 -4.0000e+02  1e-05  2e-13  2e-09\n",
            "Optimal solution found.\n",
            "     pcost       dcost       gap    pres   dres\n",
            " 0: -5.3223e+02 -1.4397e+03  5e+03  2e+00  3e-09\n",
            " 1: -3.7841e+02 -9.7795e+02  7e+02  8e-02  3e-09\n",
            " 2: -3.8470e+02 -5.5390e+02  2e+02  2e-02  2e-09\n",
            " 3: -3.8941e+02 -4.3530e+02  5e+01  2e-03  2e-09\n",
            " 4: -3.9495e+02 -4.0995e+02  2e+01  5e-04  3e-09\n",
            " 5: -3.9618e+02 -4.0680e+02  1e+01  3e-04  2e-09\n",
            " 6: -3.9812e+02 -4.0235e+02  4e+00  4e-05  2e-09\n",
            " 7: -3.9884e+02 -4.0087e+02  2e+00  1e-05  3e-09\n",
            " 8: -3.9926e+02 -4.0012e+02  9e-01  2e-06  2e-09\n",
            " 9: -3.9945e+02 -3.9983e+02  4e-01  6e-07  2e-09\n",
            "10: -3.9954e+02 -3.9971e+02  2e-01  2e-07  2e-09\n",
            "11: -3.9960e+02 -3.9962e+02  2e-02  1e-08  3e-09\n",
            "12: -3.9961e+02 -3.9961e+02  5e-03  3e-09  3e-09\n",
            "13: -3.9961e+02 -3.9961e+02  7e-04  2e-10  3e-09\n",
            "14: -3.9961e+02 -3.9961e+02  7e-06  2e-12  3e-09\n",
            "Optimal solution found.\n",
            "Test Accuracy Obatined for poly function: 36.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RBF function offered the highest accuracy of 59.33% followed by poly at 36% and then linear function with 33.33%."
      ],
      "metadata": {
        "id": "9Jii5_cUdi_N"
      }
    }
  ]
}